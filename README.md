Session 1: How to Solve the Big Data Problem
Assignment 1:

Various sources of Big Data:
1.    Social network profiles—Tapping user profiles from Facebook, LinkedIn, Yahoo, Google, and specific-interest social or travel sites, to cull individuals’ profiles and demographic information, and extend that to capture their hopefully-like-minded networks.   (This requires a fairly straightforward API integration for importing pre-defined fields and values – for example, a social network API integration that gathers every B2B marketer on Twitter.)
2.    Social influencers—Editor, analyst and subject-matter expert blog comments, user forums, Twitter & Facebook “likes,” Yelp-style catalog and review sites, and other review-centric sites like Apple’s App Store, Amazon, ZDNet, etc.   (Accessing this data requires Natural Language Processing and/or text-based search capability to evaluate the positive/negative nature of words and phrases, derive meaning, index, and write the results).
3.    Activity-generated data—Computer and mobile device log files, aka “The Internet of Things.” This category includes web site tracking information, application logs, and sensor data – such as check-ins and other location tracking – among other machine-generated content.  But consider also the data generated by the processors found within vehicles, video games, cable boxes or, soon, household appliances.  (Parsing technologies such as those from Splunk or Xenos help make sense of these types of semi-structured text files and documents.)
4.    Software as a Service (SaaS) and cloud applications—Systems like Salesforce.com, Netsuite, SuccessFactors, etc. all represent data that’s already in the Cloud but is difficult to move and merge with internal data.  (Distributed data integration technology, in-memory caching technology and API integration work may be appropriate here.)
5.    Public—Microsoft Azure MarketPlace/DataMarket, The World Bank, SEC/Edgar, Wikipedia, IMDb, etc. – data that is publicly available on the Web which may enhance the types of analysis able to be performed.  (Use the same types of parsing, usage, search and categorization techniques as for the three previously mentioned sources.)
6.    Hadoop MapReduce application results—The next generation technology architectures for handling and parallel parsing of data from logs, Web posts, etc., promise to create a new generations of pre- and post-processed data.   We foresee a ton of new products that will address application use cases for any kinds of Big Data – just look at the partner lists of Cloudera and Hortonworks.   In fact, we won’t be surprised if layers of MapReduce applications blending everything mentioned above (consolidating, “reducing” and aggregating Big Data in a layered or hierarchical approach) are very likely to become their own “Big Data”. 
7.    Data warehouse appliances—Teradata, IBM Netezza, EMC Greenplum, etc. are collecting from operational systems the internal, transactional data that is already prepared for analysis.  These will likely become an integration target that will assist in enhancing the parsed and reduced results from your Big Data installation. 
8.    Columnar/NoSQL data sources—MongoDB, Cassandra, InfoBright, etc. – examples of a new type of map reduce repository and data aggregator.  These are specialty applications that fill gaps in Hadoop-based environments, for example Cassandra’s use in collecting large volumes of real-time, distributed data.
9.    Network and in-stream monitoring technologies—Packet evaluation and distributed query processing-like applications as well as email parsers are also likely areas that will explode with new startup technologies.     
10.  Legacy documents—Archives of statements, insurance forms, medical record and customer correspondence are still an untapped resource.  (Many archives are full of old PDF documents and print streams files that contain original and only systems of record between organizations and their customers. Parsing this semi-structured legacy content can be challenging without specialty tools like Xenos.)

3 Vs of Big Data:
1. Volume: We currently see the exponential growth in the data storage as the data is now more than text data. We can find data in the format of videos, musics and large images on our social media channels. It is very common to have Terabytes and Petabytes of the storage system for enterprises. As the database grows the applications and architecture built to support the data needs to be reevaluated quite often. 
2. Variety: Data can be stored in multiple format. For example database, excel, csv, access or for the matter of the fact, it can be stored in a simple text file. Sometimes the data is not even in the traditional format as we assume, it may be in the form of video, SMS, pdf or something we might have not thought about it. It is the need of the organization to arrange it and make it meaningful. 
3. Velocity: The data growth and social media explosion have changed how we look at the data. There was a time when we used to believe that data of yesterday is recent. The matter of the fact newspapers is still following that logic. However, news channels and radios have changed how fast we receive the news. Today, people reply on social media to update them with the latest happening. 

HORIZANTAL SCALING AND VERTICAL SCALING 

Horizontal scaling means that you scale by adding more machines into your pool of resources whereas Vertical scaling means that you scale
by adding more power (CPU, RAM) to an existing machine.

Horizontal scalability is the ability to increase capacity by connecting multiple hardware or software entities so that they work as a 
single logical unit.
When servers are clustered, the original server is being scaled out horizontally. If a cluster requires more resources to improve 
performance and provide high availability (HA), an administrator can scale out by adding more servers to the cluster.
An important advantage of horizontal scalability is that it can provide administrators with the ability to increase capacity on the 
fly. Another advantage is that in theory, horizontal scalability is only limited by how many entities can be connected successfully.
The distributed storage system Cassandra, for example, runs on top of hundreds of commodity nodes spread across different data centers.
Because the commodity hardware is scaled out horizontally, Cassandra is fault tolerant and does not have a single point of failure.

Vertical scalability, on the other hand, increases capacity by adding more resources, such as more memory or an additional CPU, to a
machine. Scaling vertically, which is also called scaling up, usually requires downtime while new resources are being added and has 
limits that are defined by hardware. When Amazon RDS customers need to scale vertically, for example, they can switch from a smaller to a bigger machine, but Amazon's largest RDS instance has only 68 GB of memory.

Scaling horizontally has both advantages and disadvantages. For example, adding inexpensive commodity computers to a cluster might seem
to be a cost-effective solution at first glance, but it's important for the administrator to know whether the licensing costs for those
additional servers, the additional operations cost of powering and cooling and the large footprint they will occupy in the data center 
truly makes scaling horizontally a better choice than scaling vertically.


NEED AND WORKING OF HADOOP 


Hadoop is an open source software platform managed by the Apache Software Foundation has proven to be very helpful in storing and 
managing vast amounts of data cheaply and efficiently.
But what exactly is Hadoop, and what makes it so special? 
Basically, it’s a way of storing enormous data sets across distributed clusters of servers and then running “distributed” analysis 
applications in each cluster.
It is designed to be robust, in that your Big Data applications will continue to run even when individual servers — or clusters — fail.
And it’s also designed to be efficient, because it doesn’t require your applications to shuttle huge volumes of data across your network.
It has two main parts –
1. Data processing framework - The data processing framework is the tool used to work with the data itself. By default, this is the 
   Java-based system known as MapReduce.
2. Distributed filesystem for data storage - It is the Hadoop component that holds the actual data. By default, Hadoop uses the 
   cleverly named Hadoop Distributed File System (HDFS), although it can use other file systems as well.
   
Hadoop is not really a database: It stores data and you can pull data out of it, but there are no queries involved – SQL or otherwise. 
Hadoop is more of a data warehousing system – so it needs a system like MapReduce to actually process the data. Using MapReduce instead
of a query gives data s
Hadoop is an open source software platform managed by the Apache Software Foundation has proven to be very helpful in storing and 
managing vast amounts of data cheaply and efficiently.
But what exactly is Hadoop, and what makes it so special? 
Basically, it’s a way of storing enormous data sets across distributed clusters of servers and then running “distributed” analysis 
applications in each cluster.
It is designed to be robust, in that your Big Data applications will continue to run even when individual servers — or clusters — fail.
And it’s also designed to be efficient, because it doesn’t require your applications to shuttle huge volumes of data across your network.
It has two main parts –
1. Data processing framework - The data processing framework is the tool used to work with the data itself. By default, this is the 
   Java-based system known as MapReduce.
2. Distributed filesystem for data storage - It is the Hadoop component that holds the actual data. By default, Hadoop uses the 
   cleverly named Hadoop Distributed File System (HDFS), although it can use other file systems as well.
   
Hadoop is not really a database: It stores data and you can pull data out of it, but there are no queries involved – SQL or otherwise. 
Hadoop is more of a data warehousing system – so it needs a system like MapReduce to actually process the data. Using MapReduce instead
of a query gives data seekers a lot of power and flexibility, but also adds a lot of complexity.
eekers a lot of power and flexibility, but also adds a lot of complexity.










